{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 安装必要库\n",
    "# !pip install openai gradio\n",
    "\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 配置 vLLM 的 HTTP API 基础地址和 API 密钥\n",
    "openai_api_base = \"http://vllm-vllm.apps.cluster-q9pdq.q9pdq.sandbox1883.opentlc.com/v1\"\n",
    "openai_api_key = \"EMPTY\"  # 如果需要鉴权，替换为实际密钥\n",
    "\n",
    "# 创建 OpenAI 客户端\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "# 定义聊天函数\n",
    "def vllm_chat(user_input, history):\n",
    "    \"\"\"\n",
    "    与 vLLM 进行对话。\n",
    "    \n",
    "    参数:\n",
    "    - user_input: 用户输入的文本\n",
    "    - history: 聊天历史记录\n",
    "    \n",
    "    返回:\n",
    "    - 模型的回答，以及更新的聊天历史记录\n",
    "    \"\"\"\n",
    "    # 将聊天历史转换为消息格式\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    for user_msg, bot_msg in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # 调用 vLLM 模型\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"mistralai/Mistral-7B-Instruct-v0.2\",  # 替换为实际使用的模型\n",
    "            messages=messages\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "        bot_reply = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        bot_reply = f\"An error occurred: {str(e)}\"\n",
    "\n",
    "    # 更新聊天历史\n",
    "    history.append((user_input, bot_reply))\n",
    "    return bot_reply, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/gradio/components/chatbot.py:225: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "which: no node in (/bin:/home/qxu/.local/bin:/home/qxu/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/var/lib/snapd/snap/bin:/home/qxu/.local/bin:/home/qxu/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/var/lib/snapd/snap/bin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='cmpl-18bd91bf69254b548c241404468e4c59', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=' Hello! How can I help you today? If you have any questions or need assistance with something, please don\\'t hesitate to ask. I\\'m here to help.\\n\\nYou can ask me about a variety of topics, such as mathematics, science, English grammar, history, and more. I can also help you find information on the internet, set reminders, and answer general knowledge questions.\\n\\nIf you\\'re not quite sure what you need help with, feel free to ask me anything and I\\'ll do my best to provide you with accurate and helpful information.\\n\\nIs there a specific question or task you have in mind? Let me know and I\\'ll be happy to help!\\n\\nHere\\'s an example of a question I could help with: \"What is the capital city of France?\" The answer is: Paris.\\n\\nLet me know if you have any questions or need help with anything else!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), stop_reason=None)], created=1732091209, model='mistralai/Mistral-7B-Instruct-v0.2', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=196, prompt_tokens=18, total_tokens=214, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/blocks.py\", line 2025, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/blocks.py\", line 1831, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/components/chatbot.py\", line 532, in postprocess\n",
      "    self._check_format(value, \"tuples\")\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/components/chatbot.py\", line 331, in _check_format\n",
      "    raise Error(\n",
      "gradio.exceptions.Error: 'Data incompatible with tuples format. Each message should be a list of length 2.'\n"
     ]
    }
   ],
   "source": [
    "# 构建 Gradio 界面\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## vLLM Chatbot with Gradio\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(label=\"Your Message\", placeholder=\"Type something here...\")\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "    \n",
    "    # 更新对话历史\n",
    "    state = gr.State([])  # 聊天历史记录\n",
    "\n",
    "    # 定义交互\n",
    "    msg.submit(vllm_chat, [msg, state], [chatbot, state])\n",
    "    clear.click(lambda: ([], []), None, [chatbot, state])\n",
    "\n",
    "# 启动 Gradio 应用\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
